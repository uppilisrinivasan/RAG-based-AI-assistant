{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "hb0Nlg5ZBuPD"
   },
   "source": [
    "# ‚ö° RAG Embedding + FAISS Index (Colab GPU)\n",
    "This notebook loads customer support data, encodes queries using `sentence-transformers`, caches embeddings, builds a FAISS index, and supports basic GPT-2-based RAG querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "id": "kMwmrftwBuPG"
   },
   "outputs": [],
   "source": [
    "# üì¶ Install dependencies\n",
    "!pip install -q faiss-cpu sentence-transformers datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_jMaMcQXZNxY",
    "outputId": "78b114fb-abff-4357-998f-19e457d926bd"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "my5v82aHBuPH",
    "outputId": "f7a8ec60-a194-496b-e9e8-1f4926a43d74"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import hashlib\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import faiss\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# Cache paths\n",
    "CACHE_DIR = \"cache\"\n",
    "DATA_CACHE_FILE = os.path.join(CACHE_DIR, \"customer_support_full.csv\")\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "# Load or cache dataset\n",
    "if os.path.exists(DATA_CACHE_FILE):\n",
    "    print(f\"üîÅ Loading dataset from cache: {DATA_CACHE_FILE}\")\n",
    "    df = pd.read_csv(DATA_CACHE_FILE)\n",
    "else:\n",
    "    print(\"üåê Downloading dataset...\")\n",
    "    dataset = load_dataset(\"MohammadOthman/mo-customer-support-tweets-945k\")\n",
    "    df = pd.DataFrame({\n",
    "        \"customer_query\": dataset[\"train\"][\"input\"],\n",
    "        \"support_reply\": dataset[\"train\"][\"output\"]\n",
    "    })\n",
    "    #df = df.head(10000)  # Use subset\n",
    "    df.to_csv(DATA_CACHE_FILE, index=False)\n",
    "    print(f\"‚úÖ Cached dataset to {DATA_CACHE_FILE}\")\n",
    "\n",
    "# Embedding and FAISS setup\n",
    "class VectorStore:\n",
    "    def __init__(self, data, batch_size=128, cache_dir=\"cache/\"):\n",
    "        self.cache_dir = cache_dir\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "        self.cache_file = os.path.join(cache_dir, \"embeddings_full.npy\")\n",
    "\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"üöÄ Using device: {self.device}\")\n",
    "        self.model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=self.device)\n",
    "\n",
    "        if os.path.exists(self.cache_file):\n",
    "            print(f\"üîÅ Loading cached embeddings from {self.cache_file}...\")\n",
    "            self.embeddings = np.load(self.cache_file)\n",
    "            print(f\"‚úÖ Loaded embeddings from cache.\")\n",
    "        else:\n",
    "            print(\"‚öôÔ∏è Generating embeddings...\")\n",
    "            self.data = data\n",
    "            self.index = faiss.IndexFlatL2(384)\n",
    "\n",
    "            self.embeddings = []\n",
    "            for i in tqdm(range(0, len(data), batch_size)):\n",
    "                batch = data['customer_query'].iloc[i:i+batch_size].tolist()\n",
    "                encoded = self.model.encode(batch, show_progress_bar=False)\n",
    "                self.embeddings.extend(encoded)\n",
    "\n",
    "            self.embeddings = np.array(self.embeddings)\n",
    "            np.save(self.cache_file, self.embeddings)\n",
    "            print(f\"‚úÖ Saved embeddings to {self.cache_file}\")\n",
    "\n",
    "        self.data = data\n",
    "        self.index = faiss.IndexFlatL2(384)\n",
    "        self.index.add(self.embeddings)\n",
    "\n",
    "    def search(self, query, top_k=3):\n",
    "        query_vec = self.model.encode([query])\n",
    "        distances, indices = self.index.search(np.array(query_vec), top_k)\n",
    "        valid_indices = [i for i in indices[0] if 0 <= i < len(self.data)]\n",
    "        return [self.data.iloc[i]['support_reply'] for i in valid_indices]\n",
    "\n",
    "# üß™ Instantiate VectorStore and test\n",
    "vector_store = VectorStore(df)\n",
    "print(vector_store.search(\"I ordered a laptop, but it arrived with a broken screen. What should I do?\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MGuHJEfieej7",
    "outputId": "956bb45d-bb27-499e-d962-568b755dacaa"
   },
   "outputs": [],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138,
     "referenced_widgets": [
      "c4cc822e176045e391c489cd809ddac9",
      "3897884ede754fa68bc88eb387a6a25a",
      "2db72ef2ff114e6fba83888e0b1aea96",
      "c8fd473d57f64c2b8e298ace7850a8aa",
      "2d66d60f31e64c60b366a059bb3420d9",
      "4436e3284bff4e9ea5db5d800b54a351",
      "9b9dfa0ed7cd416dbd6da37a4c01f46e",
      "a0e87dc00f7c47c79acfb7a0738349eb",
      "0ce4b8f438b146d2b087d5e0aab5122c",
      "b33bd65867674885b7f049a3426310f4",
      "377f7364b05b4873a0bcfe47af6eb95c"
     ]
    },
    "id": "4XtfprtmBuPI",
    "outputId": "3f2535c2-1f6b-4d5c-d555-0b40ce19b2af"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Use LLaMA 2 7B Chat model (make sure you have access)\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "print(model.hf_device_map)\n",
    "\n",
    "# Pipeline setup\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# CSV log setup\n",
    "LOG_PATH = \"chat_log.csv\"\n",
    "if not os.path.exists(LOG_PATH):\n",
    "    pd.DataFrame(columns=[\"timestamp\", \"query\", \"context\", \"response\"]).to_csv(LOG_PATH, index=False)\n",
    "\n",
    "def log_interaction(query, context, response):\n",
    "    timestamp = datetime.now().isoformat()\n",
    "    new_entry = {\n",
    "        \"timestamp\": timestamp,\n",
    "        \"query\": query,\n",
    "        \"context\": context,\n",
    "        \"response\": response\n",
    "    }\n",
    "    log_df = pd.read_csv(LOG_PATH)\n",
    "    log_df = pd.concat([log_df, pd.DataFrame([new_entry])], ignore_index=True)\n",
    "    log_df.to_csv(LOG_PATH, index=False)\n",
    "    print(f\"üìù Interaction logged at {timestamp}\")\n",
    "\n",
    "def generate_response(query):\n",
    "    context = vector_store.search(query)\n",
    "    context_str = \"\\n\".join(context)\n",
    "\n",
    "    prompt = f\"\"\"<s>[INST] <<SYS>>You are a helpful support assistant.<</SYS>>\n",
    "\n",
    "    Past replies:\n",
    "    {context_str}\n",
    "\n",
    "    New question: {query}\n",
    "    Answer: [/INST]\"\"\"\n",
    "\n",
    "    # Tokenize and generate\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    start = time.time()\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"‚è±Ô∏è Time taken: {end - start:.2f} seconds\")\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    answer = response.split(\"Answer:\")[-1].strip()\n",
    "    print(\"üí¨ Response:\", answer)\n",
    "\n",
    "    # Log the interaction\n",
    "    log_interaction(query, context_str, answer)\n",
    "\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 385
    },
    "id": "x7VNpHQWBuPI",
    "outputId": "43999c9d-b1b1-412c-c94e-e5b202d2d195"
   },
   "outputs": [],
   "source": [
    "# üöÄ Try it out!\n",
    "generate_response(\"I ordered a laptop, but it arrived with a broken screen. What should I do?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EMFv7NFVKnJ-",
    "outputId": "4eb8f299-f1a6-4895-ba92-a67d889a3f4d"
   },
   "outputs": [],
   "source": [
    "!pip install fastapi uvicorn nest-asyncio pyngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "id": "yKlstneAPEH1"
   },
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, Request\n",
    "from pydantic import BaseModel\n",
    "import nest_asyncio\n",
    "from pyngrok import ngrok, conf\n",
    "import uvicorn\n",
    "from threading import Thread\n",
    "\n",
    "# Allow nested async loops (for running Uvicorn in a Jupyter notebook)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Define FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# Define request schema for the incoming query\n",
    "class QueryRequest(BaseModel):\n",
    "    query: str\n",
    "\n",
    "# Function to generate a response based on your RAG model\n",
    "#def generate_response(query: str):\n",
    "    # Replace this with your actual RAG model logic or vector store search\n",
    "    # This is where you would query your vector store and use the LLM to generate a response\n",
    "    #return f\"You can reset your password by following the instructions provided in the email sent to you.\"\n",
    "\n",
    "# Define FastAPI route to handle the query\n",
    "@app.post(\"/rag-query\")\n",
    "async def rag_query(request: QueryRequest):\n",
    "    query = request.query\n",
    "    # Use the generate_response function to get a response based on the query\n",
    "    response = generate_response(query)\n",
    "    return {\"results\": [response]}\n",
    "\n",
    "# Start the server in a thread\n",
    "def run():\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\n",
    "# Start the FastAPI server in a separate thread to avoid blocking\n",
    "Thread(target=run).start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "id": "b-w0HEHSuZDt"
   },
   "outputs": [],
   "source": [
    "#Please copy and paste your ngrok Authtoken in place of the text\n",
    "os.environ[\"NGROK_AUTHTOKEN\"] = \"Put your token here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "id": "6o9kbYybuSQh"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get authtoken from environment variable\n",
    "authtoken = os.getenv(\"NGROK_AUTHTOKEN\")\n",
    "\n",
    "if authtoken:\n",
    "    conf.get_default().auth_token = authtoken\n",
    "else:\n",
    "    raise ValueError(\"üö´ NGROK_AUTHTOKEN environment variable is not set!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ewCUimXulIl",
    "outputId": "812ea885-fa03-4c8a-84a1-408a662775a5"
   },
   "outputs": [],
   "source": [
    "conf.get_default().auth_token = authtoken\n",
    "\n",
    "# Start ngrok tunnel and print the public URL\n",
    "public_url = ngrok.connect(8000).public_url\n",
    "print(f\"üåê Public URL: {public_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zj5F5liRNJCt",
    "outputId": "89ccf367-2e10-4e83-e2a1-8c10d0922533"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Using ngrok public url to process rag query\n",
    "url = f\"{public_url}/rag-query\"\n",
    "\n",
    "# Payload with the query you want to send\n",
    "payload = {\"query\": \"I ordered a laptop, but it arrived with a broken screen. What should I do?\"}\n",
    "\n",
    "# Sending the POST request to the FastAPI endpoint\n",
    "response = requests.post(url, json=payload)\n",
    "\n",
    "# Print the response from the FastAPI server (the result of the query)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nsMxcvE-YQy2",
    "outputId": "d34e0aed-77b1-48bb-ba5e-0c4b2685b9bb"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define your queries (you can modify these or dynamically pass them based on user input)\n",
    "queries = [\n",
    "    \"I need help resetting my password.\",\n",
    "    \"I didn‚Äôt receive the reset link.\"\n",
    "]\n",
    "\n",
    "# Function to handle multi-turn queries\n",
    "def get_rag_response(query_list):\n",
    "    response = None\n",
    "    for query in query_list:\n",
    "        payload = {\"query\": query}\n",
    "        response = requests.post(url, json=payload)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"Response to '{query}': {response.json()}\")\n",
    "        else:\n",
    "            print(f\"Error in response: {response.status_code}\")\n",
    "    return response\n",
    "\n",
    "# Call the function to process the queries\n",
    "get_rag_response(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ZuFiaZ8Xhcn",
    "outputId": "3454bff6-6052-4fbb-9e34-8b3fc396d036"
   },
   "outputs": [],
   "source": [
    "# Payload with the query you want to send\n",
    "payload = {\"query\": \"My cat chewed my phone charger. Is this covered under warranty?\"}\n",
    "\n",
    "# Sending the POST request to the FastAPI endpoint\n",
    "response = requests.post(url, json=payload)\n",
    "\n",
    "# Print the response from the FastAPI server (the result of the query)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b4SCvGE5Xpr3",
    "outputId": "2a477dc4-f1b5-4cac-bc6c-bbeca174c715"
   },
   "outputs": [],
   "source": [
    "# Payload with the query you want to send\n",
    "payload = {\"query\": \"Why did you suggest contacting support?\"}\n",
    "\n",
    "# Sending the POST request to the FastAPI endpoint\n",
    "response = requests.post(url, json=payload)\n",
    "\n",
    "# Print the response from the FastAPI server (the result of the query)\n",
    "print(response.json())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
