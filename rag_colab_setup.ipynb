{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uppilisrinivasan/RAG-based-AI-assistant/blob/main/rag_colab_setup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hb0Nlg5ZBuPD"
      },
      "source": [
        "# ‚ö° RAG Embedding + FAISS Index (Colab GPU)\n",
        "This notebook loads customer support data, encodes queries using `sentence-transformers`, caches embeddings, builds a FAISS index, and supports basic GPT-2-based RAG querying."
      ],
      "id": "hb0Nlg5ZBuPD"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kMwmrftwBuPG"
      },
      "outputs": [],
      "source": [
        "# üì¶ Install dependencies\n",
        "!pip install -q faiss-cpu sentence-transformers datasets transformers"
      ],
      "id": "kMwmrftwBuPG"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"GPU device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jMaMcQXZNxY",
        "outputId": "78b114fb-abff-4357-998f-19e457d926bd"
      },
      "id": "_jMaMcQXZNxY",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "GPU device: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "my5v82aHBuPH",
        "outputId": "f7a8ec60-a194-496b-e9e8-1f4926a43d74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÅ Loading dataset from cache: cache/customer_support_full.csv\n",
            "üöÄ Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÅ Loading cached embeddings from cache/embeddings_full.npy...\n",
            "‚úÖ Loaded embeddings from cache.\n",
            "['Hello. This is definitely not the eerience we want you to have and we will do everything we can to help. To get started when you mention broken, does that mean there is physical damage to the screen?', 'That is not what we want! I am sad to hear this happened to you! Have no fear though, you have come to the right place! Can you please message me via this link so we can talk? JonathanMacInnes', 'Ah man. we would be happy to go over replacement options with you, Nina. Send a message our way DanKing']\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import hashlib\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import faiss\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "\n",
        "# Cache paths\n",
        "CACHE_DIR = \"cache\"\n",
        "DATA_CACHE_FILE = os.path.join(CACHE_DIR, \"customer_support_full.csv\")\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "\n",
        "# Load or cache dataset\n",
        "if os.path.exists(DATA_CACHE_FILE):\n",
        "    print(f\"üîÅ Loading dataset from cache: {DATA_CACHE_FILE}\")\n",
        "    df = pd.read_csv(DATA_CACHE_FILE)\n",
        "else:\n",
        "    print(\"üåê Downloading dataset...\")\n",
        "    dataset = load_dataset(\"MohammadOthman/mo-customer-support-tweets-945k\")\n",
        "    df = pd.DataFrame({\n",
        "        \"customer_query\": dataset[\"train\"][\"input\"],\n",
        "        \"support_reply\": dataset[\"train\"][\"output\"]\n",
        "    })\n",
        "    #df = df.head(10000)  # Use subset\n",
        "    df.to_csv(DATA_CACHE_FILE, index=False)\n",
        "    print(f\"‚úÖ Cached dataset to {DATA_CACHE_FILE}\")\n",
        "\n",
        "# Embedding and FAISS setup\n",
        "class VectorStore:\n",
        "    def __init__(self, data, batch_size=128, cache_dir=\"cache/\"):\n",
        "        self.cache_dir = cache_dir\n",
        "        os.makedirs(cache_dir, exist_ok=True)\n",
        "\n",
        "        self.cache_file = os.path.join(cache_dir, \"embeddings_full.npy\")\n",
        "\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        print(f\"üöÄ Using device: {self.device}\")\n",
        "        self.model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=self.device)\n",
        "\n",
        "        if os.path.exists(self.cache_file):\n",
        "            print(f\"üîÅ Loading cached embeddings from {self.cache_file}...\")\n",
        "            self.embeddings = np.load(self.cache_file)\n",
        "            print(f\"‚úÖ Loaded embeddings from cache.\")\n",
        "        else:\n",
        "            print(\"‚öôÔ∏è Generating embeddings...\")\n",
        "            self.data = data\n",
        "            self.index = faiss.IndexFlatL2(384)\n",
        "\n",
        "            self.embeddings = []\n",
        "            for i in tqdm(range(0, len(data), batch_size)):\n",
        "                batch = data['customer_query'].iloc[i:i+batch_size].tolist()\n",
        "                encoded = self.model.encode(batch, show_progress_bar=False)\n",
        "                self.embeddings.extend(encoded)\n",
        "\n",
        "            self.embeddings = np.array(self.embeddings)\n",
        "            np.save(self.cache_file, self.embeddings)\n",
        "            print(f\"‚úÖ Saved embeddings to {self.cache_file}\")\n",
        "\n",
        "        self.data = data\n",
        "        self.index = faiss.IndexFlatL2(384)\n",
        "        self.index.add(self.embeddings)\n",
        "\n",
        "    def search(self, query, top_k=3):\n",
        "        query_vec = self.model.encode([query])\n",
        "        distances, indices = self.index.search(np.array(query_vec), top_k)\n",
        "        valid_indices = [i for i in indices[0] if 0 <= i < len(self.data)]\n",
        "        return [self.data.iloc[i]['support_reply'] for i in valid_indices]\n",
        "\n",
        "# üß™ Instantiate VectorStore and test\n",
        "vector_store = VectorStore(df)\n",
        "print(vector_store.search(\"I ordered a laptop, but it arrived with a broken screen. What should I do?\"))\n",
        "\n"
      ],
      "id": "my5v82aHBuPH"
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGuHJEfieej7",
        "outputId": "956bb45d-bb27-499e-d962-568b755dacaa"
      },
      "id": "MGuHJEfieej7",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `celonis_challenge_hugging_face_token1` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `celonis_challenge_hugging_face_token1`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "c4cc822e176045e391c489cd809ddac9",
            "3897884ede754fa68bc88eb387a6a25a",
            "2db72ef2ff114e6fba83888e0b1aea96",
            "c8fd473d57f64c2b8e298ace7850a8aa",
            "2d66d60f31e64c60b366a059bb3420d9",
            "4436e3284bff4e9ea5db5d800b54a351",
            "9b9dfa0ed7cd416dbd6da37a4c01f46e",
            "a0e87dc00f7c47c79acfb7a0738349eb",
            "0ce4b8f438b146d2b087d5e0aab5122c",
            "b33bd65867674885b7f049a3426310f4",
            "377f7364b05b4873a0bcfe47af6eb95c"
          ]
        },
        "id": "4XtfprtmBuPI",
        "outputId": "3f2535c2-1f6b-4d5c-d555-0b40ce19b2af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py:897: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c4cc822e176045e391c489cd809ddac9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'': 0}\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "import time\n",
        "import pandas as pd\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Use LLaMA 2 7B Chat model (make sure you have access)\n",
        "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "# Load model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "\n",
        "print(model.hf_device_map)\n",
        "\n",
        "# Pipeline setup\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# CSV log setup\n",
        "LOG_PATH = \"chat_log.csv\"\n",
        "if not os.path.exists(LOG_PATH):\n",
        "    pd.DataFrame(columns=[\"timestamp\", \"query\", \"context\", \"response\"]).to_csv(LOG_PATH, index=False)\n",
        "\n",
        "def log_interaction(query, context, response):\n",
        "    timestamp = datetime.now().isoformat()\n",
        "    new_entry = {\n",
        "        \"timestamp\": timestamp,\n",
        "        \"query\": query,\n",
        "        \"context\": context,\n",
        "        \"response\": response\n",
        "    }\n",
        "    log_df = pd.read_csv(LOG_PATH)\n",
        "    log_df = pd.concat([log_df, pd.DataFrame([new_entry])], ignore_index=True)\n",
        "    log_df.to_csv(LOG_PATH, index=False)\n",
        "    print(f\"üìù Interaction logged at {timestamp}\")\n",
        "\n",
        "def generate_response(query):\n",
        "    context = vector_store.search(query)\n",
        "    context_str = \"\\n\".join(context)\n",
        "\n",
        "    prompt = f\"\"\"<s>[INST] <<SYS>>You are a helpful support assistant.<</SYS>>\n",
        "\n",
        "    Past replies:\n",
        "    {context_str}\n",
        "\n",
        "    New question: {query}\n",
        "    Answer: [/INST]\"\"\"\n",
        "\n",
        "    # Tokenize and generate\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    start = time.time()\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=200,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9\n",
        "    )\n",
        "    end = time.time()\n",
        "\n",
        "    print(f\"‚è±Ô∏è Time taken: {end - start:.2f} seconds\")\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    answer = response.split(\"Answer:\")[-1].strip()\n",
        "    print(\"üí¨ Response:\", answer)\n",
        "\n",
        "    # Log the interaction\n",
        "    log_interaction(query, context_str, answer)\n",
        "\n",
        "    return answer\n"
      ],
      "id": "4XtfprtmBuPI"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "x7VNpHQWBuPI",
        "outputId": "43999c9d-b1b1-412c-c94e-e5b202d2d195"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è±Ô∏è Time taken: 13.22 seconds\n",
            "üí¨ Response: [/INST]  Oh no, I'm so sorry to hear that your laptop arrived with a broken screen! üòî That can be very frustrating.\n",
            "\n",
            "Firstly, please don't worry, you've come to the right place! We'll do our best to help you resolve this issue as quickly as possible. üòä\n",
            "\n",
            "Can you please provide me with some more details about the situation? For example, when did you place the order, and did you notice any damage during the shipping process? Any information you can provide will help us to better understand the situation and find the best solution for you. ü§î\n",
            "\n",
            "Additionally, have you tried contacting the seller or manufacturer directly to see if they can provide any assistance? They may be able to offer a repair or replacement option for you. üì≤\n",
            "\n",
            "Please let me know if there's anything else you need help with,\n",
            "üìù Interaction logged at 2025-04-16T08:40:18.837226\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"[/INST]  Oh no, I'm so sorry to hear that your laptop arrived with a broken screen! üòî That can be very frustrating.\\n\\nFirstly, please don't worry, you've come to the right place! We'll do our best to help you resolve this issue as quickly as possible. üòä\\n\\nCan you please provide me with some more details about the situation? For example, when did you place the order, and did you notice any damage during the shipping process? Any information you can provide will help us to better understand the situation and find the best solution for you. ü§î\\n\\nAdditionally, have you tried contacting the seller or manufacturer directly to see if they can provide any assistance? They may be able to offer a repair or replacement option for you. üì≤\\n\\nPlease let me know if there's anything else you need help with,\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# üöÄ Try it out!\n",
        "generate_response(\"I ordered a laptop, but it arrived with a broken screen. What should I do?\")"
      ],
      "id": "x7VNpHQWBuPI"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi uvicorn nest-asyncio pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMFv7NFVKnJ-",
        "outputId": "4eb8f299-f1a6-4895-ba92-a67d889a3f4d"
      },
      "id": "EMFv7NFVKnJ-",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (0.115.12)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (0.34.1)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.4)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (0.46.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.11.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.13.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.0)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, Request\n",
        "from pydantic import BaseModel\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok, conf\n",
        "import uvicorn\n",
        "from threading import Thread\n",
        "\n",
        "# Allow nested async loops (for running Uvicorn in a Jupyter notebook)\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Define FastAPI app\n",
        "app = FastAPI()\n",
        "\n",
        "# Define request schema for the incoming query\n",
        "class QueryRequest(BaseModel):\n",
        "    query: str\n",
        "\n",
        "# Function to generate a response based on your RAG model\n",
        "#def generate_response(query: str):\n",
        "    # Replace this with your actual RAG model logic or vector store search\n",
        "    # This is where you would query your vector store and use the LLM to generate a response\n",
        "    #return f\"You can reset your password by following the instructions provided in the email sent to you.\"\n",
        "\n",
        "# Define FastAPI route to handle the query\n",
        "@app.post(\"/rag-query\")\n",
        "async def rag_query(request: QueryRequest):\n",
        "    query = request.query\n",
        "    # Use the generate_response function to get a response based on the query\n",
        "    response = generate_response(query)\n",
        "    return {\"results\": [response]}\n",
        "\n",
        "# Start the server in a thread\n",
        "def run():\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "\n",
        "# Start the FastAPI server in a separate thread to avoid blocking\n",
        "Thread(target=run).start()\n"
      ],
      "metadata": {
        "id": "yKlstneAPEH1"
      },
      "id": "yKlstneAPEH1",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Please copy and paste your ngrok Authtoken in place of the text\n",
        "os.environ[\"NGROK_AUTHTOKEN\"] = \"Put your token here\""
      ],
      "metadata": {
        "id": "b-w0HEHSuZDt"
      },
      "id": "b-w0HEHSuZDt",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Get authtoken from environment variable\n",
        "authtoken = os.getenv(\"NGROK_AUTHTOKEN\")\n",
        "\n",
        "if authtoken:\n",
        "    conf.get_default().auth_token = authtoken\n",
        "else:\n",
        "    raise ValueError(\"üö´ NGROK_AUTHTOKEN environment variable is not set!\")"
      ],
      "metadata": {
        "id": "6o9kbYybuSQh"
      },
      "id": "6o9kbYybuSQh",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conf.get_default().auth_token = authtoken\n",
        "\n",
        "# Start ngrok tunnel and print the public URL\n",
        "public_url = ngrok.connect(8000).public_url\n",
        "print(f\"üåê Public URL: {public_url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ewCUimXulIl",
        "outputId": "812ea885-fa03-4c8a-84a1-408a662775a5"
      },
      "id": "0ewCUimXulIl",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üåê Public URL: https://6fe6-34-125-27-125.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# Using ngrok public url to process rag query\n",
        "url = f\"{public_url}/rag-query\"\n",
        "\n",
        "# Payload with the query you want to send\n",
        "payload = {\"query\": \"I ordered a laptop, but it arrived with a broken screen. What should I do?\"}\n",
        "\n",
        "# Sending the POST request to the FastAPI endpoint\n",
        "response = requests.post(url, json=payload)\n",
        "\n",
        "# Print the response from the FastAPI server (the result of the query)\n",
        "print(response.json())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zj5F5liRNJCt",
        "outputId": "89ccf367-2e10-4e83-e2a1-8c10d0922533"
      },
      "id": "Zj5F5liRNJCt",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è±Ô∏è Time taken: 9.44 seconds\n",
            "üí¨ Response: [/INST]  Oh no, I'm so sorry to hear that your laptop arrived with a broken screen! üòî That can be really frustrating and disappointing.\n",
            "\n",
            "Firstly, please don't worry, you've come to the right place! We'll do everything we can to help you resolve this issue as quickly and efficiently as possible. üôÇ\n",
            "\n",
            "Can you please provide me with some more details? Did you receive any damage notice or confirmation from the seller before receiving the laptop? And have you tried contacting them yet to report the issue? ü§î\n",
            "\n",
            "We'll need to gather some information to help you with your next steps. üòä\n",
            "üìù Interaction logged at 2025-04-16T08:45:37.537862\n",
            "INFO:     34.125.27.125:0 - \"POST /rag-query HTTP/1.1\" 200 OK\n",
            "{'results': [\"[/INST]  Oh no, I'm so sorry to hear that your laptop arrived with a broken screen! üòî That can be really frustrating and disappointing.\\n\\nFirstly, please don't worry, you've come to the right place! We'll do everything we can to help you resolve this issue as quickly and efficiently as possible. üôÇ\\n\\nCan you please provide me with some more details? Did you receive any damage notice or confirmation from the seller before receiving the laptop? And have you tried contacting them yet to report the issue? ü§î\\n\\nWe'll need to gather some information to help you with your next steps. üòä\"]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define your queries (you can modify these or dynamically pass them based on user input)\n",
        "queries = [\n",
        "    \"I need help resetting my password.\",\n",
        "    \"I didn‚Äôt receive the reset link.\"\n",
        "]\n",
        "\n",
        "# Function to handle multi-turn queries\n",
        "def get_rag_response(query_list):\n",
        "    response = None\n",
        "    for query in query_list:\n",
        "        payload = {\"query\": query}\n",
        "        response = requests.post(url, json=payload)\n",
        "        if response.status_code == 200:\n",
        "            print(f\"Response to '{query}': {response.json()}\")\n",
        "        else:\n",
        "            print(f\"Error in response: {response.status_code}\")\n",
        "    return response\n",
        "\n",
        "# Call the function to process the queries\n",
        "get_rag_response(queries)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsMxcvE-YQy2",
        "outputId": "d34e0aed-77b1-48bb-ba5e-0c4b2685b9bb"
      },
      "id": "nsMxcvE-YQy2",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è±Ô∏è Time taken: 2.92 seconds\n",
            "üí¨ Response: [/INST]  Of course, I'd be happy to help you reset your password! üòä Can you please provide me with the email address associated with your account, so I can guide you through the password reset process?\n",
            "üìù Interaction logged at 2025-04-16T08:45:44.604249\n",
            "INFO:     34.125.27.125:0 - \"POST /rag-query HTTP/1.1\" 200 OK\n",
            "Response to 'I need help resetting my password.': {'results': [\"[/INST]  Of course, I'd be happy to help you reset your password! üòä Can you please provide me with the email address associated with your account, so I can guide you through the password reset process?\"]}\n",
            "‚è±Ô∏è Time taken: 3.91 seconds\n",
            "üí¨ Response: [/INST]  Of course, I'd be happy to help! Can you please provide me with more details about the issue you're experiencing? For example, did you receive an error message or is the link not working? Additionally, can you please confirm the email address you used to request the reset link?\n",
            "üìù Interaction logged at 2025-04-16T08:45:48.916416\n",
            "INFO:     34.125.27.125:0 - \"POST /rag-query HTTP/1.1\" 200 OK\n",
            "Response to 'I didn‚Äôt receive the reset link.': {'results': [\"[/INST]  Of course, I'd be happy to help! Can you please provide me with more details about the issue you're experiencing? For example, did you receive an error message or is the link not working? Additionally, can you please confirm the email address you used to request the reset link?\"]}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Response [200]>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Payload with the query you want to send\n",
        "payload = {\"query\": \"My cat chewed my phone charger. Is this covered under warranty?\"}\n",
        "\n",
        "# Sending the POST request to the FastAPI endpoint\n",
        "response = requests.post(url, json=payload)\n",
        "\n",
        "# Print the response from the FastAPI server (the result of the query)\n",
        "print(response.json())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZuFiaZ8Xhcn",
        "outputId": "3454bff6-6052-4fbb-9e34-8b3fc396d036"
      },
      "id": "3ZuFiaZ8Xhcn",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è±Ô∏è Time taken: 12.78 seconds\n",
            "üí¨ Response: [/INST]  Of course, I'd be happy to help you with that! üòä\n",
            "\n",
            "Unfortunately, Apple's warranty does not cover damage caused by external factors such as chewing. So, I'm afraid your charger is not covered under warranty if it has been damaged by your cat. üòû\n",
            "\n",
            "However, you may be able to purchase a replacement charger from Apple or an authorized reseller. They offer a variety of chargers that are compatible with your iPhone, and they may have a specific charger that is designed to be more durable and resistant to chewing. ü§î\n",
            "\n",
            "Additionally, you may want to consider purchasing a protective case for your charger to help prevent any further damage. There are many different cases available that are specifically designed to protect your charger from scratches, drops, and other types of damage. üí°\n",
            "üìù Interaction logged at 2025-04-16T08:46:05.051008\n",
            "INFO:     34.125.27.125:0 - \"POST /rag-query HTTP/1.1\" 200 OK\n",
            "{'results': [\"[/INST]  Of course, I'd be happy to help you with that! üòä\\n\\nUnfortunately, Apple's warranty does not cover damage caused by external factors such as chewing. So, I'm afraid your charger is not covered under warranty if it has been damaged by your cat. üòû\\n\\nHowever, you may be able to purchase a replacement charger from Apple or an authorized reseller. They offer a variety of chargers that are compatible with your iPhone, and they may have a specific charger that is designed to be more durable and resistant to chewing. ü§î\\n\\nAdditionally, you may want to consider purchasing a protective case for your charger to help prevent any further damage. There are many different cases available that are specifically designed to protect your charger from scratches, drops, and other types of damage. üí°\"]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Payload with the query you want to send\n",
        "payload = {\"query\": \"Why did you suggest contacting support?\"}\n",
        "\n",
        "# Sending the POST request to the FastAPI endpoint\n",
        "response = requests.post(url, json=payload)\n",
        "\n",
        "# Print the response from the FastAPI server (the result of the query)\n",
        "print(response.json())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4SCvGE5Xpr3",
        "outputId": "2a477dc4-f1b5-4cac-bc6c-bbeca174c715"
      },
      "id": "b4SCvGE5Xpr3",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è±Ô∏è Time taken: 9.63 seconds\n",
            "üí¨ Response: [/INST]  Ah, I see! You are asking about the reason why I suggested contacting support. Well, Desi, our team is here to help you with any questions or concerns you may have. Whether it's about your account, a problem you're facing, or just a general question, we are here to assist you.\n",
            "\n",
            "By contacting support, you will be able to reach out to our team directly and get the help you need. Our team is trained to provide you with the best possible assistance, and we will do our best to resolve any issue you may have as quickly and efficiently as possible.\n",
            "\n",
            "So, if you have any questions or concerns, please don't hesitate to reach out to us. We are here to help!\n",
            "üìù Interaction logged at 2025-04-16T08:46:16.453744\n",
            "INFO:     34.125.27.125:0 - \"POST /rag-query HTTP/1.1\" 200 OK\n",
            "{'results': [\"[/INST]  Ah, I see! You are asking about the reason why I suggested contacting support. Well, Desi, our team is here to help you with any questions or concerns you may have. Whether it's about your account, a problem you're facing, or just a general question, we are here to assist you.\\n\\nBy contacting support, you will be able to reach out to our team directly and get the help you need. Our team is trained to provide you with the best possible assistance, and we will do our best to resolve any issue you may have as quickly and efficiently as possible.\\n\\nSo, if you have any questions or concerns, please don't hesitate to reach out to us. We are here to help!\"]}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c4cc822e176045e391c489cd809ddac9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3897884ede754fa68bc88eb387a6a25a",
              "IPY_MODEL_2db72ef2ff114e6fba83888e0b1aea96",
              "IPY_MODEL_c8fd473d57f64c2b8e298ace7850a8aa"
            ],
            "layout": "IPY_MODEL_2d66d60f31e64c60b366a059bb3420d9"
          }
        },
        "3897884ede754fa68bc88eb387a6a25a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4436e3284bff4e9ea5db5d800b54a351",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9b9dfa0ed7cd416dbd6da37a4c01f46e",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "2db72ef2ff114e6fba83888e0b1aea96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0e87dc00f7c47c79acfb7a0738349eb",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0ce4b8f438b146d2b087d5e0aab5122c",
            "value": 2
          }
        },
        "c8fd473d57f64c2b8e298ace7850a8aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b33bd65867674885b7f049a3426310f4",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_377f7364b05b4873a0bcfe47af6eb95c",
            "value": "‚Äá2/2‚Äá[00:56&lt;00:00,‚Äá25.87s/it]"
          }
        },
        "2d66d60f31e64c60b366a059bb3420d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4436e3284bff4e9ea5db5d800b54a351": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b9dfa0ed7cd416dbd6da37a4c01f46e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0e87dc00f7c47c79acfb7a0738349eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ce4b8f438b146d2b087d5e0aab5122c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b33bd65867674885b7f049a3426310f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "377f7364b05b4873a0bcfe47af6eb95c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}